{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "qmdDiV2PGARv",
        "YILvUA_rGT9c",
        "eVW2j0xZHLlu",
        "3qLv5kduHQYw",
        "hWGB6lwjHWna",
        "2agEYAYnHcL7",
        "arn2EWkiHgXu",
        "itmOTNW8HlzY",
        "4eCXRwLwUcz-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Group Information: To be filled by the candidates.\n",
        "\n",
        "### Group Number: 14\n",
        "### Members Roll Numbers: 24AI60R13, 24AI60R46\n",
        "### Members Name: Vinayak Mali, Saurabh Jaiswal\n",
        "\n"
      ],
      "metadata": {
        "id": "Dd3YcdHFFV6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Pre-requist"
      ],
      "metadata": {
        "id": "qmdDiV2PGARv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlJdNrHkFQhL"
      },
      "outputs": [],
      "source": [
        "!pip install pykeen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from pykeen.datasets import Nations, Kinships"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeTS6iS6GE55",
        "outputId": "6bf6bab3-5b02-4ffe-e2f9-ad5ac74f2d5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pykeen.utils:Using opt_einsum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Set random seeds\")\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS12R5V5GHKh",
        "outputId": "acb3806f-44dd-4236-d943-d2278e0681e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set random seeds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"device check\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycfyWG4eGNJk",
        "outputId": "9c897a2f-7a98-4e45-a28c-1c823b136875"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device check\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Loading"
      ],
      "metadata": {
        "id": "YILvUA_rGT9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Nations and Kinships datasets\n",
        "print(\"Loading datasets...\")\n",
        "nations_dataset = Nations()\n",
        "kinships_dataset = Kinships()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIb6ncArGVrX",
        "outputId": "e1b20824-666b-4346-9078-072a577f1204"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting training, validation, and test triples for both datasets\")\n",
        "nations_train = nations_dataset.training.mapped_triples.tolist()\n",
        "nations_test = nations_dataset.testing.mapped_triples.tolist()\n",
        "kinships_train = kinships_dataset.training.mapped_triples.tolist()\n",
        "kinships_test = kinships_dataset.testing.mapped_triples.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPASzuFcMRh_",
        "outputId": "751df84f-de24-41db-ada1-bc50c2677ecc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting training, validation, and test triples for both datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Getting entity and relation counts\")\n",
        "nations_num_entities = nations_dataset.num_entities\n",
        "nations_num_relations = nations_dataset.num_relations\n",
        "kinships_num_entities = kinships_dataset.num_entities\n",
        "kinships_num_relations = kinships_dataset.num_relations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKqyMUNSMjBv",
        "outputId": "396a430c-df86-4ee0-9f01-f9c39d355a89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting entity and relation counts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Nations Dataset - Entities: {nations_num_entities}, Relations: {nations_num_relations}\")\n",
        "print(f\"Kinships Dataset - Entities: {kinships_num_entities}, Relations: {kinships_num_relations}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTX7sWwVMo7M",
        "outputId": "7906775e-8849-492a-ab6c-afc43aeb175e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nations Dataset - Entities: 14, Relations: 55\n",
            "Kinships Dataset - Entities: 104, Relations: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Triples"
      ],
      "metadata": {
        "id": "eVW2j0xZHLlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Function for extracting triples from datasets\")\n",
        "def extract_triples(triples):\n",
        "    return [(h, r, t) for h, r, t in triples]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VOb42YfXaFH",
        "outputId": "6b41677b-222a-43f7-f9a0-237fafd79746"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function for extracting triples from datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting triples from Nations and Kinships datasets\")\n",
        "nations_train_triples = extract_triples(nations_train)\n",
        "nations_test_triples = extract_triples(nations_test)\n",
        "kinships_train_triples = extract_triples(kinships_train)\n",
        "kinships_test_triples = extract_triples(kinships_test)"
      ],
      "metadata": {
        "id": "mGxkNew_HLU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62289642-4fdf-4ea4-dd41-b436d7358ac7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting triples from Nations and Kinships datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data preprocessing and triple extraction complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCOrPGXlM3m5",
        "outputId": "eb8099a8-98f7-4662-b62e-60950ff09263"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing and triple extraction complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. TransE & transR\n"
      ],
      "metadata": {
        "id": "3qLv5kduHQYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining the TransE model class\")\n",
        "class TransE(nn.Module):\n",
        "    def __init__(self, num_entities, num_relations, embedding_dim, margin, norm=1):\n",
        "        super(TransE, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.margin = margin\n",
        "        self.norm = norm\n",
        "        # Initialize entity and relation embeddings\n",
        "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
        "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
        "        # Initialize embeddings\n",
        "        nn.init.uniform_(self.entity_embeddings.weight.data, -6/np.sqrt(embedding_dim), 6/np.sqrt(embedding_dim))\n",
        "        nn.init.uniform_(self.relation_embeddings.weight.data, -6/np.sqrt(embedding_dim), 6/np.sqrt(embedding_dim))\n",
        "\n",
        "    def forward(self, head, relation, tail):\n",
        "        # Get embeddings for head, relation, and tail\n",
        "        head_emb = self.entity_embeddings(head)\n",
        "        rel_emb = self.relation_embeddings(relation)\n",
        "        tail_emb = self.entity_embeddings(tail)\n",
        "\n",
        "        # Calculate score based on the TransE distance function\n",
        "        score = torch.norm(head_emb + rel_emb - tail_emb, p=self.norm, dim=1)\n",
        "        return score\n",
        "\n",
        "    def get_embeddings(self):\n",
        "        # Return entity and relation embeddings\n",
        "        return self.entity_embeddings.weight, self.relation_embeddings.weight\n"
      ],
      "metadata": {
        "id": "jOZ7cwmqHShq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692a307e-540b-463d-b87b-115b8bf8b0e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining the TransE model class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Defining the TransR model class\")\n",
        "class TransR(nn.Module):\n",
        "    def __init__(self, num_entities, num_relations, embedding_dim, margin, rel_embedding_dim):\n",
        "        super(TransR, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.rel_embedding_dim = rel_embedding_dim\n",
        "        self.margin = margin\n",
        "        # Initialize entity and relation embeddings\n",
        "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
        "        self.relation_embeddings = nn.Embedding(num_relations, rel_embedding_dim)\n",
        "        # Transformation matrix for each relation\n",
        "        self.transformation_matrices = nn.Embedding(num_relations, rel_embedding_dim * embedding_dim)\n",
        "\n",
        "        # Initialize embeddings\n",
        "        nn.init.uniform_(self.entity_embeddings.weight.data, -6/np.sqrt(embedding_dim), 6/np.sqrt(embedding_dim))\n",
        "        nn.init.uniform_(self.relation_embeddings.weight.data, -6/np.sqrt(rel_embedding_dim), 6/np.sqrt(rel_embedding_dim))\n",
        "        nn.init.xavier_uniform_(self.transformation_matrices.weight.data)\n",
        "\n",
        "    def forward(self, head, relation, tail):\n",
        "        # Get embeddings\n",
        "        head_emb = self.entity_embeddings(head)\n",
        "        tail_emb = self.entity_embeddings(tail)\n",
        "        rel_emb = self.relation_embeddings(relation)\n",
        "        # Get the transformation matrix for the relation\n",
        "        transformation_matrix = self.transformation_matrices(relation).view(-1, self.rel_embedding_dim, self.embedding_dim)\n",
        "        # Project entities into relation-specific space\n",
        "        head_proj = torch.bmm(transformation_matrix, head_emb.unsqueeze(-1)).squeeze(-1)\n",
        "        tail_proj = torch.bmm(transformation_matrix, tail_emb.unsqueeze(-1)).squeeze(-1)\n",
        "        # Calculate score\n",
        "        score = torch.norm(head_proj + rel_emb - tail_proj, p=1, dim=1)\n",
        "        return score\n",
        "\n",
        "    def get_embeddings(self):\n",
        "        # Return entity and relation embeddings\n",
        "        return self.entity_embeddings.weight, self.relation_embeddings.weight\n"
      ],
      "metadata": {
        "id": "bR3nY_9rNZT9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d695b9-45ad-4833-bc2d-de766a4c2ff3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining the TransR model class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Bernoulli Negative Sampling"
      ],
      "metadata": {
        "id": "hWGB6lwjHWna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bernoulli Negative Sampling\")\n",
        "def bernoulli_negative_sample(triples, num_entities, num_samples=1):\n",
        "    # This function generates negative samples by randomly corrupting heads or tails\n",
        "    negative_samples = []\n",
        "    for head, relation, tail in triples:\n",
        "        for _ in range(num_samples):\n",
        "            if np.random.rand() < 0.5:  # Replace head with random entity\n",
        "                neg_head = np.random.randint(0, num_entities)\n",
        "                while neg_head == head:\n",
        "                    neg_head = np.random.randint(0, num_entities)\n",
        "                negative_samples.append((neg_head, relation, tail))\n",
        "            else:  # Replace tail with random entity\n",
        "                neg_tail = np.random.randint(0, num_entities)\n",
        "                while neg_tail == tail:\n",
        "                    neg_tail = np.random.randint(0, num_entities)\n",
        "                negative_samples.append((head, relation, neg_tail))\n",
        "    return negative_samples"
      ],
      "metadata": {
        "id": "yaNydRsIHYWL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Margin Ranking Loss\")\n",
        "def margin_ranking_loss(pos_score, neg_score, margin):\n",
        "    return torch.mean(torch.clamp(margin + pos_score - neg_score, min=0))"
      ],
      "metadata": {
        "id": "Kj4w4s2DN2sJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training"
      ],
      "metadata": {
        "id": "2agEYAYnHcL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate models and define training parameters\n",
        "embedding_dim = 100\n",
        "rel_embedding_dim = 100  # For TransR\n",
        "margin_values = [1.0, 2.0, 3.0, 4.0, 5.0]  # Varying margin values\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n",
        "\n",
        "print(\"Models initialized with embedding dimension 100 and margin 1.0.\")\n",
        "\n",
        "# Sample initialization for TransE and TransR with a sample margin\n",
        "transe_model = TransE(nations_num_entities, nations_num_relations, embedding_dim, margin=1.0).to(device)\n",
        "transr_model = TransR(nations_num_entities, nations_num_relations, embedding_dim, margin=1.0, rel_embedding_dim=rel_embedding_dim).to(device)\n",
        "\n",
        "# Define optimizers\n",
        "optimizer_transe = optim.Adam(transe_model.parameters(), lr=learning_rate)\n",
        "optimizer_transr = optim.Adam(transr_model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buQ8MGZ9OBm_",
        "outputId": "285aceb7-fab2-45c3-8ca8-95d68ba641dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models initialized with embedding dimension 100 and margin 1.0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function for TransE and TransR with Bernoulli Negative Sampling\n",
        "def train_model(model, optimizer, criterion, train_triples, num_entities, batch_size, num_epochs, device, margin):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        np.random.shuffle(train_triples)  # Shuffle triples for each epoch\n",
        "        total_loss = 0\n",
        "\n",
        "        # Batch training\n",
        "        for i in range(0, len(train_triples), batch_size):\n",
        "            batch_triples = train_triples[i:i + batch_size]\n",
        "            pos_head, pos_rel, pos_tail = zip(*batch_triples)\n",
        "\n",
        "            # Convert to torch tensors\n",
        "            pos_head = torch.tensor(pos_head).to(device)\n",
        "            pos_rel = torch.tensor(pos_rel).to(device)\n",
        "            pos_tail = torch.tensor(pos_tail).to(device)\n",
        "\n",
        "            # Generate negative samples using Bernoulli Negative Sampling\n",
        "            neg_triples = bernoulli_negative_sample(batch_triples, num_entities)\n",
        "            neg_head, neg_rel, neg_tail = zip(*neg_triples)\n",
        "            neg_head = torch.tensor(neg_head).to(device)\n",
        "            neg_rel = torch.tensor(neg_rel).to(device)\n",
        "            neg_tail = torch.tensor(neg_tail).to(device)\n",
        "\n",
        "            # Calculate scores for positive and negative samples\n",
        "            pos_score = model(pos_head, pos_rel, pos_tail)\n",
        "            neg_score = model(neg_head, neg_rel, neg_tail)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(pos_score, neg_score, margin)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "RKE5rOJZHc6E"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation Function"
      ],
      "metadata": {
        "id": "arn2EWkiHgXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function for 1-hop question-answering\n",
        "from pykeen.evaluation import RankBasedEvaluator\n",
        "\n",
        "def evaluate_model(model, test_triples, device, num_entities):\n",
        "    model.eval()\n",
        "    evaluator = RankBasedEvaluator()\n",
        "    metrics = {'mean_rank': [], 'hits_at_10': []}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for head, rel, tail in test_triples:\n",
        "            # Head prediction\n",
        "            head_predictions = []\n",
        "            for candidate in range(num_entities):\n",
        "                score = model(torch.tensor([candidate]).to(device),\n",
        "                              torch.tensor([rel]).to(device),\n",
        "                              torch.tensor([tail]).to(device))\n",
        "                head_predictions.append((candidate, score.item()))\n",
        "            head_predictions = sorted(head_predictions, key=lambda x: x[1])\n",
        "            correct_head_rank = next(i for i, (entity, _) in enumerate(head_predictions) if entity == head)\n",
        "            metrics['mean_rank'].append(correct_head_rank + 1)\n",
        "            metrics['hits_at_10'].append(1 if correct_head_rank < 10 else 0)\n",
        "\n",
        "            # Tail prediction\n",
        "            tail_predictions = []\n",
        "            for candidate in range(num_entities):\n",
        "                score = model(torch.tensor([head]).to(device),\n",
        "                              torch.tensor([rel]).to(device),\n",
        "                              torch.tensor([candidate]).to(device))\n",
        "                tail_predictions.append((candidate, score.item()))\n",
        "            tail_predictions = sorted(tail_predictions, key=lambda x: x[1])\n",
        "            correct_tail_rank = next(i for i, (entity, _) in enumerate(tail_predictions) if entity == tail)\n",
        "            metrics['mean_rank'].append(correct_tail_rank + 1)\n",
        "            metrics['hits_at_10'].append(1 if correct_tail_rank < 10 else 0)\n",
        "\n",
        "    # Calculate Mean Rank and Hits@10\n",
        "    mean_rank = np.mean(metrics['mean_rank'])\n",
        "    hits_at_10 = np.mean(metrics['hits_at_10'])\n",
        "\n",
        "    print(f\"Evaluation Results: Mean Rank (MR) = {mean_rank}, Hits@10 = {hits_at_10 * 100}%\")\n"
      ],
      "metadata": {
        "id": "MSmSbz5XHhJ9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Execution"
      ],
      "metadata": {
        "id": "itmOTNW8HlzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, criterion, and train TransE with margin 1.0 as a demonstration\n",
        "margin = 1.0\n",
        "criterion = margin_ranking_loss"
      ],
      "metadata": {
        "id": "7eFtTRvmG6Rx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(transe_model, optimizer_transe, criterion, nations_train_triples, nations_num_entities, batch_size=128, num_epochs=num_epochs, device=device, margin=margin)\n",
        "# Example evaluation call on Nations dataset test triples\n",
        "print(\"Evaluating TransE model...\")\n",
        "evaluate_model(transe_model, nations_test_triples, device, nations_num_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOAmVsloT7Fo",
        "outputId": "48d1b3e4-67ff-4cd8-c667-eaf666321d51"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 45.4096\n",
            "Epoch [2/50], Loss: 42.9939\n",
            "Epoch [3/50], Loss: 42.3667\n",
            "Epoch [4/50], Loss: 40.3386\n",
            "Epoch [5/50], Loss: 38.3300\n",
            "Epoch [6/50], Loss: 39.7635\n",
            "Epoch [7/50], Loss: 34.2934\n",
            "Epoch [8/50], Loss: 36.5350\n",
            "Epoch [9/50], Loss: 32.3080\n",
            "Epoch [10/50], Loss: 33.4884\n",
            "Epoch [11/50], Loss: 34.5222\n",
            "Epoch [12/50], Loss: 32.1814\n",
            "Epoch [13/50], Loss: 30.2607\n",
            "Epoch [14/50], Loss: 30.6974\n",
            "Epoch [15/50], Loss: 30.4464\n",
            "Epoch [16/50], Loss: 29.4711\n",
            "Epoch [17/50], Loss: 28.0881\n",
            "Epoch [18/50], Loss: 26.9708\n",
            "Epoch [19/50], Loss: 24.7332\n",
            "Epoch [20/50], Loss: 27.7638\n",
            "Epoch [21/50], Loss: 28.8775\n",
            "Epoch [22/50], Loss: 25.7822\n",
            "Epoch [23/50], Loss: 26.3975\n",
            "Epoch [24/50], Loss: 25.4755\n",
            "Epoch [25/50], Loss: 26.9676\n",
            "Epoch [26/50], Loss: 25.5370\n",
            "Epoch [27/50], Loss: 25.5878\n",
            "Epoch [28/50], Loss: 23.7035\n",
            "Epoch [29/50], Loss: 23.8597\n",
            "Epoch [30/50], Loss: 21.6219\n",
            "Epoch [31/50], Loss: 21.8558\n",
            "Epoch [32/50], Loss: 22.1228\n",
            "Epoch [33/50], Loss: 21.6265\n",
            "Epoch [34/50], Loss: 22.0621\n",
            "Epoch [35/50], Loss: 23.6934\n",
            "Epoch [36/50], Loss: 21.5604\n",
            "Epoch [37/50], Loss: 21.4083\n",
            "Epoch [38/50], Loss: 20.5644\n",
            "Epoch [39/50], Loss: 18.7590\n",
            "Epoch [40/50], Loss: 20.8838\n",
            "Epoch [41/50], Loss: 20.2653\n",
            "Epoch [42/50], Loss: 18.5840\n",
            "Epoch [43/50], Loss: 19.2277\n",
            "Epoch [44/50], Loss: 19.4736\n",
            "Epoch [45/50], Loss: 18.9402\n",
            "Epoch [46/50], Loss: 18.7486\n",
            "Epoch [47/50], Loss: 17.4425\n",
            "Epoch [48/50], Loss: 18.2594\n",
            "Epoch [49/50], Loss: 16.4388\n",
            "Epoch [50/50], Loss: 16.8001\n",
            "Evaluating TransE model...\n",
            "Evaluation Results: Mean Rank (MR) = 8.037313432835822, Hits@10 = 72.636815920398%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example training call for TransR model on Nations dataset with margin 1.0\n",
        "train_model(transr_model, optimizer_transr, criterion, nations_train_triples, nations_num_entities, batch_size=128, num_epochs=num_epochs, device=device, margin=margin)\n",
        "# Evaluation for TransR\n",
        "print(\"Evaluating TransR model...\")\n",
        "evaluate_model(transr_model, nations_test_triples, device, nations_num_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9f5ruEyUKZA",
        "outputId": "2913a33d-4895-4f8f-89ba-ae2fd478f1ed"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 20.0288\n",
            "Epoch [2/50], Loss: 18.6452\n",
            "Epoch [3/50], Loss: 18.3549\n",
            "Epoch [4/50], Loss: 15.7413\n",
            "Epoch [5/50], Loss: 16.4710\n",
            "Epoch [6/50], Loss: 15.4581\n",
            "Epoch [7/50], Loss: 14.8644\n",
            "Epoch [8/50], Loss: 14.7310\n",
            "Epoch [9/50], Loss: 13.9629\n",
            "Epoch [10/50], Loss: 13.5664\n",
            "Epoch [11/50], Loss: 12.4545\n",
            "Epoch [12/50], Loss: 12.7124\n",
            "Epoch [13/50], Loss: 12.8867\n",
            "Epoch [14/50], Loss: 12.0041\n",
            "Epoch [15/50], Loss: 12.2193\n",
            "Epoch [16/50], Loss: 12.7536\n",
            "Epoch [17/50], Loss: 10.8840\n",
            "Epoch [18/50], Loss: 11.2657\n",
            "Epoch [19/50], Loss: 11.3740\n",
            "Epoch [20/50], Loss: 12.1895\n",
            "Epoch [21/50], Loss: 12.0831\n",
            "Epoch [22/50], Loss: 11.5057\n",
            "Epoch [23/50], Loss: 10.7513\n",
            "Epoch [24/50], Loss: 10.6494\n",
            "Epoch [25/50], Loss: 11.3159\n",
            "Epoch [26/50], Loss: 10.5647\n",
            "Epoch [27/50], Loss: 10.3132\n",
            "Epoch [28/50], Loss: 10.3277\n",
            "Epoch [29/50], Loss: 10.8625\n",
            "Epoch [30/50], Loss: 11.0124\n",
            "Epoch [31/50], Loss: 10.9242\n",
            "Epoch [32/50], Loss: 10.2241\n",
            "Epoch [33/50], Loss: 10.7497\n",
            "Epoch [34/50], Loss: 10.2484\n",
            "Epoch [35/50], Loss: 9.9953\n",
            "Epoch [36/50], Loss: 9.8692\n",
            "Epoch [37/50], Loss: 10.2051\n",
            "Epoch [38/50], Loss: 9.7245\n",
            "Epoch [39/50], Loss: 9.4854\n",
            "Epoch [40/50], Loss: 10.3907\n",
            "Epoch [41/50], Loss: 9.5305\n",
            "Epoch [42/50], Loss: 9.5408\n",
            "Epoch [43/50], Loss: 9.6404\n",
            "Epoch [44/50], Loss: 9.5868\n",
            "Epoch [45/50], Loss: 9.8636\n",
            "Epoch [46/50], Loss: 9.5818\n",
            "Epoch [47/50], Loss: 10.0169\n",
            "Epoch [48/50], Loss: 9.4845\n",
            "Epoch [49/50], Loss: 9.4740\n",
            "Epoch [50/50], Loss: 9.0756\n",
            "Evaluating TransR model...\n",
            "Evaluation Results: Mean Rank (MR) = 8.062189054726367, Hits@10 = 70.39800995024875%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Similar Fact Retrieval Task"
      ],
      "metadata": {
        "id": "4eCXRwLwUcz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "RYNniJ71iKyv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Some declartion and embeddings\")\n",
        "validation_triples = [\n",
        "    ('brazil', 'commonbloc1', 'india'),\n",
        "    ('burma', 'intergovorgs3', 'indonesia'),\n",
        "    ('china', 'accusation', 'uk'),\n",
        "    ('cuba', 'reldiplomacy', 'china'),\n",
        "    ('egypt', 'embassy', 'uk')\n",
        "]\n",
        "\n",
        "embedding_dim = 30\n",
        "rel_embedding_dim = 30\n",
        "transe_model = TransE(nations_num_entities, nations_num_relations, embedding_dim, margin=1.0).to(device)\n",
        "transr_model = TransR(nations_num_entities, nations_num_relations, embedding_dim, margin=1.0, rel_embedding_dim=rel_embedding_dim).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaQXcqzUa6Eq",
        "outputId": "78de0968-7e88-404c-ffd0-79650c3b3cd4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some declartion and embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('COnverting nations entity to id')\n",
        "def entity_to_id_nations(nations_dataset):\n",
        "  entity_to_id = {entity: entity_id for entity_id, entity in enumerate(nations_dataset.entity_to_id)}\n",
        "  return entity_to_id\n",
        "\n",
        "print('COnverting nations relation to id')\n",
        "def relation_to_id_nations(nations_dataset):\n",
        "  relation_to_id = {relation: relation_id for relation_id, relation in enumerate(nations_dataset.relation_to_id)}\n",
        "  return relation_to_id\n",
        "\n",
        "nations_entity_to_id = entity_to_id_nations(nations_dataset)\n",
        "nations_relation_to_id = relation_to_id_nations(nations_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXdFSXopiMhH",
        "outputId": "c6b31970-f38e-47c5-b410-976d9e7b4abe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COnverting nations entity to id\n",
            "COnverting nations relation to id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validating is id generated propelry\n",
        "print(\"nations_entity_to_id:\",nations_entity_to_id)\n",
        "print(\"nations_relation_to_id:\",nations_relation_to_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSXw-SvgeI3B",
        "outputId": "b789e43b-b5ee-4426-efb9-72ba8b1e8411"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nations_entity_to_id: {'brazil': 0, 'burma': 1, 'china': 2, 'cuba': 3, 'egypt': 4, 'india': 5, 'indonesia': 6, 'israel': 7, 'jordan': 8, 'netherlands': 9, 'poland': 10, 'uk': 11, 'usa': 12, 'ussr': 13}\n",
            "nations_relation_to_id: {'accusation': 0, 'aidenemy': 1, 'attackembassy': 2, 'blockpositionindex': 3, 'booktranslations': 4, 'boycottembargo': 5, 'commonbloc0': 6, 'commonbloc1': 7, 'commonbloc2': 8, 'conferences': 9, 'dependent': 10, 'duration': 11, 'economicaid': 12, 'eemigrants': 13, 'embassy': 14, 'emigrants3': 15, 'expeldiplomats': 16, 'exportbooks': 17, 'exports3': 18, 'independence': 19, 'intergovorgs': 20, 'intergovorgs3': 21, 'lostterritory': 22, 'militaryactions': 23, 'militaryalliance': 24, 'negativebehavior': 25, 'negativecomm': 26, 'ngo': 27, 'ngoorgs3': 28, 'nonviolentbehavior': 29, 'officialvisits': 30, 'pprotests': 31, 'relbooktranslations': 32, 'reldiplomacy': 33, 'releconomicaid': 34, 'relemigrants': 35, 'relexportbooks': 36, 'relexports': 37, 'relintergovorgs': 38, 'relngo': 39, 'relstudents': 40, 'reltourism': 41, 'reltreaties': 42, 'severdiplomatic': 43, 'students': 44, 'timesinceally': 45, 'timesincewar': 46, 'tourism': 47, 'tourism3': 48, 'treaties': 49, 'unoffialacts': 50, 'unweightedunvote': 51, 'violentactions': 52, 'warning': 53, 'weightedunvote': 54}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Retrieve all triples from the Nations dataset.\")\n",
        "def get_all_triples(nations_dataset):\n",
        "    all_triples = []\n",
        "    for triple in nations_dataset.training.mapped_triples.tolist():\n",
        "        head_id, rel_id, tail_id = triple\n",
        "        head = list(nations_entity_to_id.keys())[list(nations_entity_to_id.values()).index(head_id)]\n",
        "        relation = list(nations_relation_to_id.keys())[list(nations_relation_to_id.values()).index(rel_id)]\n",
        "        tail = list(nations_entity_to_id.keys())[list(nations_entity_to_id.values()).index(tail_id)]\n",
        "        all_triples.append((head, relation, tail))\n",
        "    return all_triples\n",
        "\n",
        "print(\"Retrieve similar triples for each validation triple.\")\n",
        "def retrieve_similar_triples(model, validation_triples, all_triples, entity_to_id, relation_to_id, k=5):\n",
        "    similar_triples = {}\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for val_triple in validation_triples:\n",
        "            print(f\"Processing validation triple: {val_triple}\")\n",
        "            val_triple_emb = get_triple_embedding(model, val_triple, entity_to_id, relation_to_id)\n",
        "            scores = []\n",
        "\n",
        "            for train_triple in all_triples:\n",
        "                if train_triple == tuple(val_triple):\n",
        "                    continue\n",
        "\n",
        "                train_triple_emb = get_triple_embedding(model, train_triple, entity_to_id, relation_to_id)\n",
        "                if val_triple_emb is not None and train_triple_emb is not None:\n",
        "                    score = calculate_similarity_score(val_triple_emb, train_triple_emb)\n",
        "                    scores.append((train_triple, score))\n",
        "\n",
        "            top_k_similar = sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
        "            similar_triples[tuple(val_triple)] = top_k_similar\n",
        "\n",
        "    return similar_triples\n",
        "\n",
        "print(\"Get the embedding of a triple using the given model.\")\n",
        "def get_triple_embedding(model, triple, entity_to_id, relation_to_id):\n",
        "    head, relation, tail = triple\n",
        "\n",
        "    try:\n",
        "        head_id = entity_to_id[head]\n",
        "        rel_id = relation_to_id[relation]\n",
        "        tail_id = entity_to_id[tail]\n",
        "    except KeyError as e:\n",
        "        print(f\"KeyError for triple {triple}: {e}\")\n",
        "        return None\n",
        "\n",
        "    head_id = torch.tensor([head_id]).to(device)\n",
        "    rel_id = torch.tensor([rel_id]).to(device)\n",
        "    tail_id = torch.tensor([tail_id]).to(device)\n",
        "\n",
        "    if isinstance(model, TransR):\n",
        "        # Get embeddings\n",
        "        head_emb = model.entity_embeddings(head_id)\n",
        "        tail_emb = model.entity_embeddings(tail_id)\n",
        "        rel_emb = model.relation_embeddings(rel_id)\n",
        "\n",
        "        # Get transformation matrix\n",
        "        transformation_matrix = model.transformation_matrices(rel_id).view(\n",
        "            -1, model.rel_embedding_dim, model.embedding_dim)\n",
        "\n",
        "        # Project entities into relation space\n",
        "        head_proj = torch.bmm(transformation_matrix, head_emb.unsqueeze(-1)).squeeze(-1)\n",
        "        tail_proj = torch.bmm(transformation_matrix, tail_emb.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        # Combine embeddings in relation space\n",
        "        triple_embedding = torch.cat([head_proj.squeeze(0), rel_emb.squeeze(0), tail_proj.squeeze(0)])\n",
        "    # STarting code for TransE\n",
        "    else:\n",
        "        head_emb = model.entity_embeddings(head_id)\n",
        "        rel_emb = model.relation_embeddings(rel_id)\n",
        "        tail_emb = model.entity_embeddings(tail_id)\n",
        "        triple_embedding = torch.cat([head_emb.squeeze(0), rel_emb.squeeze(0), tail_emb.squeeze(0)])\n",
        "\n",
        "    return triple_embedding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_HNWG0-evh7",
        "outputId": "91dfa509-0937-42a5-be79-ef98ed3ddbbc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieve all triples from the Nations dataset.\n",
            "Retrieve similar triples for each validation triple.\n",
            "Get the embedding of a triple using the given model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calculate the dot product-based similarity score between two triple embeddings.\")\n",
        "def calculate_similarity_score(triple_emb_1, triple_emb_2):\n",
        "    triple_emb_1_norm = triple_emb_1 / (torch.norm(triple_emb_1) + 1e-8)\n",
        "    triple_emb_2_norm = triple_emb_2 / (torch.norm(triple_emb_2) + 1e-8)\n",
        "    return torch.dot(triple_emb_1_norm, triple_emb_2_norm).item()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mocdwrOZi5Vx",
        "outputId": "ca1dc204-cf04-4292-9dba-a54fd2f0d5c5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculate the dot product-based similarity score between two triple embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Getting all triples from the dataset...\")\n",
        "all_triples = get_all_triples(nations_dataset)\n",
        "print(f\"Total number of triples: {len(all_triples)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AeHIOs4jqAD",
        "outputId": "a55aa189-b327-4509-a14a-4efe3e565d70"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting all triples from the dataset...\n",
            "Total number of triples: 1592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSE\n",
        "print(\"\\nFinding similar triples using TransE...\")\n",
        "similar_triples_transe = retrieve_similar_triples(\n",
        "    transe_model,\n",
        "    validation_triples,\n",
        "    all_triples,\n",
        "    nations_entity_to_id,\n",
        "    nations_relation_to_id,\n",
        "    k=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-kwhYryjq9_",
        "outputId": "8e8a29ff-ef87-4f64-b898-5663cb0c847c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding similar triples using TransE...\n",
            "Processing validation triple: ('brazil', 'commonbloc1', 'india')\n",
            "Processing validation triple: ('burma', 'intergovorgs3', 'indonesia')\n",
            "Processing validation triple: ('china', 'accusation', 'uk')\n",
            "Processing validation triple: ('cuba', 'reldiplomacy', 'china')\n",
            "Processing validation triple: ('egypt', 'embassy', 'uk')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRANSR\n",
        "print(\"\\nFinding similar triples using TransR...\")\n",
        "similar_triples_transr = retrieve_similar_triples(\n",
        "    transr_model,\n",
        "    validation_triples,\n",
        "    all_triples,\n",
        "    nations_entity_to_id,\n",
        "    nations_relation_to_id,\n",
        "    k=5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHNVJnf_jrzd",
        "outputId": "93840314-7375-4e5d-ab16-ce560835a2da"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Finding similar triples using TransR...\n",
            "Processing validation triple: ('brazil', 'commonbloc1', 'india')\n",
            "Processing validation triple: ('burma', 'intergovorgs3', 'indonesia')\n",
            "Processing validation triple: ('china', 'accusation', 'uk')\n",
            "Processing validation triple: ('cuba', 'reldiplomacy', 'china')\n",
            "Processing validation triple: ('egypt', 'embassy', 'uk')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTop 5 similar triples for each validation triple using TransE:\")\n",
        "for val_triple, sim_triples in similar_triples_transe.items():\n",
        "    print(f\"\\nValidation triple: {val_triple}\")\n",
        "    for triple, score in sim_triples:\n",
        "        print(f\"Similar triple: {triple}, Similarity score: {score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JepEYkwqi79P",
        "outputId": "17000558-e59f-4436-c50f-c07b2663b79a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 similar triples for each validation triple using TransE:\n",
            "\n",
            "Validation triple: ('brazil', 'commonbloc1', 'india')\n",
            "Similar triple: ('ussr', 'commonbloc1', 'india'), Similarity score: 0.8340\n",
            "Similar triple: ('brazil', 'relintergovorgs', 'india'), Similarity score: 0.7764\n",
            "Similar triple: ('brazil', 'commonbloc1', 'burma'), Similarity score: 0.7740\n",
            "Similar triple: ('brazil', 'ngoorgs3', 'india'), Similarity score: 0.7702\n",
            "Similar triple: ('brazil', 'commonbloc1', 'egypt'), Similarity score: 0.7494\n",
            "\n",
            "Validation triple: ('burma', 'intergovorgs3', 'indonesia')\n",
            "Similar triple: ('india', 'intergovorgs3', 'indonesia'), Similarity score: 0.7536\n",
            "Similar triple: ('burma', 'intergovorgs3', 'usa'), Similarity score: 0.7249\n",
            "Similar triple: ('burma', 'exports3', 'indonesia'), Similarity score: 0.7104\n",
            "Similar triple: ('burma', 'intergovorgs3', 'uk'), Similarity score: 0.6988\n",
            "Similar triple: ('burma', 'relngo', 'indonesia'), Similarity score: 0.6881\n",
            "\n",
            "Validation triple: ('china', 'accusation', 'uk')\n",
            "Similar triple: ('china', 'blockpositionindex', 'uk'), Similarity score: 0.7470\n",
            "Similar triple: ('china', 'accusation', 'indonesia'), Similarity score: 0.7130\n",
            "Similar triple: ('china', 'accusation', 'ussr'), Similarity score: 0.7038\n",
            "Similar triple: ('china', 'accusation', 'usa'), Similarity score: 0.6921\n",
            "Similar triple: ('china', 'relngo', 'uk'), Similarity score: 0.6734\n",
            "\n",
            "Validation triple: ('cuba', 'reldiplomacy', 'china')\n",
            "Similar triple: ('cuba', 'timesinceally', 'china'), Similarity score: 0.8051\n",
            "Similar triple: ('cuba', 'reldiplomacy', 'poland'), Similarity score: 0.7666\n",
            "Similar triple: ('cuba', 'reldiplomacy', 'uk'), Similarity score: 0.7535\n",
            "Similar triple: ('cuba', 'reldiplomacy', 'indonesia'), Similarity score: 0.7265\n",
            "Similar triple: ('poland', 'reldiplomacy', 'china'), Similarity score: 0.6966\n",
            "\n",
            "Validation triple: ('egypt', 'embassy', 'uk')\n",
            "Similar triple: ('egypt', 'dependent', 'uk'), Similarity score: 0.7750\n",
            "Similar triple: ('egypt', 'booktranslations', 'uk'), Similarity score: 0.7588\n",
            "Similar triple: ('egypt', 'unweightedunvote', 'uk'), Similarity score: 0.7583\n",
            "Similar triple: ('egypt', 'timesincewar', 'uk'), Similarity score: 0.7548\n",
            "Similar triple: ('egypt', 'intergovorgs', 'uk'), Similarity score: 0.7484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\nTop 5 similar triples for each validation triple using TransR:\")\n",
        "for val_triple, sim_triples in similar_triples_transr.items():\n",
        "    print(f\"\\nValidation triple: {val_triple}\")\n",
        "    for triple, score in sim_triples:\n",
        "        print(f\"Similar triple: {triple}, Similarity score: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaXLc2ogjSZN",
        "outputId": "3926a722-e6bd-4dff-db40-388318237f62"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 similar triples for each validation triple using TransR:\n",
            "\n",
            "Validation triple: ('brazil', 'commonbloc1', 'india')\n",
            "Similar triple: ('poland', 'commonbloc1', 'india'), Similarity score: 0.9609\n",
            "Similar triple: ('brazil', 'commonbloc1', 'burma'), Similarity score: 0.9560\n",
            "Similar triple: ('brazil', 'commonbloc1', 'jordan'), Similarity score: 0.9515\n",
            "Similar triple: ('usa', 'commonbloc1', 'india'), Similarity score: 0.9479\n",
            "Similar triple: ('ussr', 'commonbloc1', 'india'), Similarity score: 0.9450\n",
            "\n",
            "Validation triple: ('burma', 'intergovorgs3', 'indonesia')\n",
            "Similar triple: ('india', 'intergovorgs3', 'indonesia'), Similarity score: 0.9319\n",
            "Similar triple: ('burma', 'intergovorgs3', 'india'), Similarity score: 0.9312\n",
            "Similar triple: ('burma', 'intergovorgs3', 'brazil'), Similarity score: 0.9286\n",
            "Similar triple: ('burma', 'intergovorgs3', 'uk'), Similarity score: 0.9222\n",
            "Similar triple: ('burma', 'intergovorgs3', 'usa'), Similarity score: 0.9130\n",
            "\n",
            "Validation triple: ('china', 'accusation', 'uk')\n",
            "Similar triple: ('china', 'accusation', 'usa'), Similarity score: 0.9678\n",
            "Similar triple: ('china', 'accusation', 'india'), Similarity score: 0.9620\n",
            "Similar triple: ('china', 'accusation', 'indonesia'), Similarity score: 0.9608\n",
            "Similar triple: ('india', 'accusation', 'uk'), Similarity score: 0.9568\n",
            "Similar triple: ('china', 'accusation', 'ussr'), Similarity score: 0.9546\n",
            "\n",
            "Validation triple: ('cuba', 'reldiplomacy', 'china')\n",
            "Similar triple: ('cuba', 'reldiplomacy', 'indonesia'), Similarity score: 0.9731\n",
            "Similar triple: ('cuba', 'reldiplomacy', 'netherlands'), Similarity score: 0.9683\n",
            "Similar triple: ('ussr', 'reldiplomacy', 'china'), Similarity score: 0.9648\n",
            "Similar triple: ('cuba', 'reldiplomacy', 'uk'), Similarity score: 0.9640\n",
            "Similar triple: ('egypt', 'reldiplomacy', 'china'), Similarity score: 0.9614\n",
            "\n",
            "Validation triple: ('egypt', 'embassy', 'uk')\n",
            "Similar triple: ('poland', 'embassy', 'uk'), Similarity score: 0.9573\n",
            "Similar triple: ('egypt', 'embassy', 'china'), Similarity score: 0.9572\n",
            "Similar triple: ('egypt', 'embassy', 'usa'), Similarity score: 0.9499\n",
            "Similar triple: ('israel', 'embassy', 'uk'), Similarity score: 0.9492\n",
            "Similar triple: ('egypt', 'embassy', 'india'), Similarity score: 0.9432\n"
          ]
        }
      ]
    }
  ]
}